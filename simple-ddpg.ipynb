{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import pommerman\n",
    "from pommerman import agents\n",
    "from tqdm import tqdm\n",
    "xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'PommeFFACompetition-v0'\n",
    "EPISODES = 100000\n",
    "TEST = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.core import Env\n",
    "\n",
    "from pommerman.configs import ffa_competition_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper(Env):\n",
    "    \"\"\"The abstract environment class that is used by all agents. This class has the exact\n",
    "        same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n",
    "        OpenAI Gym implementation, this class only defines the abstract methods without any actual\n",
    "        implementation.\n",
    "        To implement your own environment, you need to define the following methods:\n",
    "        - `step`\n",
    "        - `reset`\n",
    "        - `render`\n",
    "        - `close`\n",
    "        Refer to the [Gym documentation](https://gym.openai.com/docs/#environments).\n",
    "        \"\"\"\n",
    "    reward_range = (-1, 1)\n",
    "    action_space = None\n",
    "    observation_space = None\n",
    "\n",
    "    def __init__(self, gym, board_size):\n",
    "        self.gym = gym\n",
    "        self.action_space = gym.action_space\n",
    "        self.observation_space = gym.observation_space\n",
    "        self.reward_range = gym.reward_range\n",
    "        self.board_size = board_size\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        # Arguments\n",
    "            action (object): An action provided by the environment.\n",
    "        # Returns\n",
    "            observation (object): Agent's observation of the current environment.\n",
    "            reward (float) : Amount of reward returned after previous action.\n",
    "            done (boolean): Whether the episode has ended, in which case further step() calls will return undefined results.\n",
    "            info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n",
    "        \"\"\"\n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, action)\n",
    "        state, reward, terminal, info = self.gym.step(all_actions)\n",
    "        agent_state = self.featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "        return agent_state, agent_reward, terminal, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment and returns an initial observation.\n",
    "        # Returns\n",
    "            observation (object): The initial observation of the space. Initial reward is assumed to be 0.\n",
    "        \"\"\"\n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = self.featurize(obs[self.gym.training_agent])\n",
    "        return agent_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.)\n",
    "        # Arguments\n",
    "            mode (str): The mode to render with.\n",
    "            close (bool): Close all open renderings.\n",
    "        \"\"\"\n",
    "        self.gym.render(mode=mode, close=close)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Override in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        self.gym.close()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        # Returns\n",
    "            Returns the list of seeds used in this env's random number generators\n",
    "        \"\"\"\n",
    "        raise self.gym.seed(seed)\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        \"\"\"Provides runtime configuration to the environment.\n",
    "        This configuration should consist of data that tells your\n",
    "        environment how to run (such as an address of a remote server,\n",
    "        or path to your ImageNet data). It should not affect the\n",
    "        semantics of the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def featurize(self, obs):\n",
    "        shape = (self.board_size, self.board_size, 1)\n",
    "\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(shape).astype(np.float32)\n",
    "\n",
    "        def get_map(board, item):\n",
    "            map = np.zeros(shape)\n",
    "            map[board == item] = 1\n",
    "            return map\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "\n",
    "        # TODO: probably not needed Passage = 0\n",
    "        rigid_map = get_map(board, 1)  # Rigid = 1\n",
    "        wood_map = get_map(board, 2)  # Wood = 2\n",
    "        bomb_map = get_map(board, 3)  # Bomb = 3\n",
    "        flames_map = get_map(board, 4)  # Flames = 4\n",
    "        fog_map = get_map(board, 5)  # TODO: not used for first two stages Fog = 5\n",
    "        extra_bomb_map = get_map(board, 6)  # ExtraBomb = 6\n",
    "        incr_range_map = get_map(board, 7)  # IncrRange = 7\n",
    "        kick_map = get_map(board, 8)  # Kick = 8\n",
    "        skull_map = get_map(board, 9)  # Skull = 9\n",
    "\n",
    "        position = obs[\"position\"]\n",
    "        my_position = np.zeros(shape)\n",
    "        my_position[position[0], position[1], 0] = 1\n",
    "\n",
    "        team_mates = get_map(board, obs[\"teammate\"].value)  # TODO during documentation it should be an array\n",
    "\n",
    "        enemies = np.zeros(shape)\n",
    "        for enemy in obs[\"enemies\"]:\n",
    "            enemies[board == enemy.value] = 1\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "\n",
    "        ammo = np.full(shape, obs[\"ammo\"])\n",
    "        blast_strength = np.full(shape, obs[\"blast_strength\"])\n",
    "        can_kick = np.full(shape, int(obs[\"can_kick\"]))\n",
    "\n",
    "        obs = np.concatenate([my_position, enemies, team_mates, rigid_map,\n",
    "                              wood_map, bomb_map, flames_map,\n",
    "                              fog_map, extra_bomb_map, incr_range_map,\n",
    "                              kick_map, skull_map, bomb_blast_strength,\n",
    "                              bomb_life, ammo, blast_strength, can_kick], axis=2)\n",
    "        return obs\n",
    "\n",
    "    ## state_to_matrix(obs)\n",
    "#     def featurize(self, obs):\n",
    "#         #In this implementation I just concatenate everything in one big matrix\n",
    "\n",
    "#         #for e in obs['enemies']:\n",
    "#         #    print(e)\n",
    "#         #    print(Item(e))\n",
    "#         #TODO enemies\n",
    "#         my_position = np.asmatrix(obs['position'])\n",
    "#         bomb_life = np.asmatrix(obs['bomb_life'])\n",
    "#         board = np.asmatrix(obs['board'])\n",
    "#         bombs = np.asmatrix(obs['bomb_blast_strength'])\n",
    "#         #enemies = np.asmatrix([Item_en(e) for e in obs['enemies']])\n",
    "#         can_kick = np.asmatrix(int(1 if obs['can_kick'] else 0))\n",
    "#         ammo = np.asmatrix(int(obs['ammo']))\n",
    "#         blast_strength = np.asmatrix(int(obs['blast_strength']))\n",
    "\n",
    "#         m = np.max([my_position.shape[1], bomb_life.shape[1], board.shape[1], bombs.shape[1],  can_kick.shape[1], ammo.shape[1], blast_strength.shape[1]])\n",
    "\n",
    "#         my_position1 = np.concatenate((my_position, np.zeros(( my_position.shape[0], m - my_position.shape[1]))), axis=1)\n",
    "#         bomb_life1 = np.concatenate((bomb_life, np.zeros((bomb_life.shape[0], m - bomb_life.shape[1]))), axis=1)\n",
    "#         board1 = np.concatenate((board, np.zeros((board.shape[0], m - board.shape[1]))), axis=1)\n",
    "#         bombs1 = np.concatenate((bombs, np.zeros((bombs.shape[0], m - bombs.shape[1]))), axis=1)\n",
    "#         #enemies1 = np.concatenate((enemies, np.zeros((enemies.shape[0], m - enemies.shape[1]))), axis=1)\n",
    "#         can_kick1 = np.concatenate((can_kick, np.zeros((can_kick.shape[0], m - can_kick.shape[1]))), axis=1)\n",
    "#         ammo1 = np.concatenate((ammo, np.zeros((ammo.shape[0], m - ammo.shape[1]))), axis=1)\n",
    "#         blast_strength1 = np.concatenate((blast_strength, np.zeros((blast_strength.shape[0], m - blast_strength.shape[1]))), axis=1)\n",
    "\n",
    "#         result = np.concatenate((my_position1, bomb_life1, board1, bombs1, can_kick1, ammo1, blast_strength1), axis=0)\n",
    "#         return np.asmatrix(result)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-18 13:17:30,744] Making new env: PommeFFACompetition-v0\n"
     ]
    }
   ],
   "source": [
    "# Create a set of agents (exactly four)\n",
    "agent_list = [\n",
    "    agents.SimpleAgent(),\n",
    "    agents.SimpleAgent(),\n",
    "    agents.RandomAgent(),\n",
    "    agents.RandomAgent(),\n",
    "]\n",
    "env = pommerman.make(ENV_NAME, agent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_agents(agent_list)\n",
    "env.set_training_agent(agent_list[-1].agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(env.training_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 11\n",
    "env_wrapper = EnvWrapper(env, BOARD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = env_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sum = np.zeros(1)\n",
    "\n",
    "from gym.wrappers import Monitor\n",
    "import datetime\n",
    "\n",
    "ACTOR_LEARNING_RATE = 0.0001\n",
    "CRITIC_LEARNING_RATE = 0.001\n",
    "MAX_EPISODES = 100000\n",
    "MAX_STEPS_EPISODE = 50000\n",
    "WARMUP_STEPS = 10000\n",
    "EXPLORATION_EPISODES = 10000\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "BUFFER_SIZE = 1000000\n",
    "OU_THETA = 0.15\n",
    "OU_MU = 0.\n",
    "OU_SIGMA = 0.3\n",
    "MIN_EPSILON = 0.1\n",
    "MAX_EPSILON = 1\n",
    "EVAL_PERIODS = 100\n",
    "EVAL_EPISODES = 10\n",
    "MINI_BATCH = 64\n",
    "RANDOM_SEED = 123\n",
    "DATETIME = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "SUMMARY_DIR = './results/{}/{}/tf_ddpg'.format(ENV_NAME, DATETIME)\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_matrix(obs):\n",
    "    #In this implementation I just concatenate everything in one big matrix\n",
    "\n",
    "    #for e in obs['enemies']:\n",
    "    #    print(e)\n",
    "    #    print(Item(e))\n",
    "    #TODO enemies\n",
    "    my_position = np.asmatrix(obs['position'])\n",
    "    bomb_life = np.asmatrix(obs['bomb_life'])\n",
    "    board = np.asmatrix(obs['board'])\n",
    "    bombs = np.asmatrix(obs['bomb_blast_strength'])\n",
    "    #enemies = np.asmatrix([Item_en(e) for e in obs['enemies']])\n",
    "    can_kick = np.asmatrix(int(1 if obs['can_kick'] else 0))\n",
    "    ammo = np.asmatrix(int(obs['ammo']))\n",
    "    blast_strength = np.asmatrix(int(obs['blast_strength']))\n",
    "\n",
    "    m = np.max([my_position.shape[1], bomb_life.shape[1], board.shape[1], bombs.shape[1],  can_kick.shape[1], ammo.shape[1], blast_strength.shape[1]])\n",
    "\n",
    "    my_position1 = np.concatenate((my_position, np.zeros(( my_position.shape[0], m - my_position.shape[1]))), axis=1)\n",
    "    bomb_life1 = np.concatenate((bomb_life, np.zeros((bomb_life.shape[0], m - bomb_life.shape[1]))), axis=1)\n",
    "    board1 = np.concatenate((board, np.zeros((board.shape[0], m - board.shape[1]))), axis=1)\n",
    "    bombs1 = np.concatenate((bombs, np.zeros((bombs.shape[0], m - bombs.shape[1]))), axis=1)\n",
    "    #enemies1 = np.concatenate((enemies, np.zeros((enemies.shape[0], m - enemies.shape[1]))), axis=1)\n",
    "    can_kick1 = np.concatenate((can_kick, np.zeros((can_kick.shape[0], m - can_kick.shape[1]))), axis=1)\n",
    "    ammo1 = np.concatenate((ammo, np.zeros((ammo.shape[0], m - ammo.shape[1]))), axis=1)\n",
    "    blast_strength1 = np.concatenate((blast_strength, np.zeros((blast_strength.shape[0], m - blast_strength.shape[1]))), axis=1)\n",
    "\n",
    "    result = np.concatenate((my_position1, bomb_life1, board1, bombs1, can_kick1, ammo1, blast_strength1), axis=0)\n",
    "    return np.asmatrix(result)\n",
    "\n",
    "def fully_connected(inputs, output_size, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(),\\\n",
    "        weights_regularizer=tf.contrib.layers.l2_regularizer(0.001), biases_initializer=tf.constant_initializer(0.0)):\n",
    "    return tf.contrib.layers.fully_connected(inputs, output_size, activation_fn=activation_fn, \\\n",
    "            weights_initializer=weights_initializer, weights_regularizer=weights_regularizer, biases_initializer=biases_initializer)\n",
    "\n",
    "def batch_norm(inputs, phase):\n",
    "    return tf.contrib.layers.batch_norm(inputs, center=True, scale=True, is_training=phase)\n",
    "\n",
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'terminal', 'next_state'])\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed=1234):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        # Right side of deque contains newest experience\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, state, action, reward, terminal, next_state):\n",
    "        experience = Transition(state, action, reward, terminal, next_state)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        return map(np.array, zip(*batch))\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "        \n",
    "def discretize(value, num_actions):\n",
    "    discretization = tf.round(value)\n",
    "    discretization = tf.minimum(tf.constant(num_actions-1, dtype=tf.float32), tf.maximum(tf.constant(0, dtype=tf.float32), tf.to_float(discretization)))\n",
    "    return tf.to_int32(discretization)\n",
    "\n",
    "class OrnsteinUhlenbeckProcess(object):\n",
    "    def __init__(self, theta, mu=0, sigma=1, x0=0, dt=1e-2, n_steps_annealing=100, size=1):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.n_steps_annealing = n_steps_annealing\n",
    "        self.sigma_step = - self.sigma / float(self.n_steps_annealing)\n",
    "        self.x0 = x0\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.size = size\n",
    "\n",
    "    def generate(self, step):\n",
    "        sigma = max(0, self.sigma_step * step + self.sigma)\n",
    "        x = self.x0 + self.theta * (self.mu - self.x0) * self.dt + sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x0 = x\n",
    "        return x\n",
    "\n",
    "class GreedyPolicy(object):\n",
    "    def __init__(self, action_dim, n_steps_annealing, min_epsilon, max_epsilon):\n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.action_dim = action_dim\n",
    "        self.n_steps_annealing = n_steps_annealing\n",
    "        self.epsilon_step = - (self.epsilon - self.min_epsilon) / float(self.n_steps_annealing)\n",
    "\n",
    "    def generate(self, action, step):\n",
    "        epsilon = max(self.min_epsilon, self.epsilon_step * step + self.epsilon)\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(range(self.action_dim))\n",
    "        else:\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNetwork(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau):\n",
    "        \"\"\"\n",
    "        base network for actor and critic network.\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            state_dim: env.observation_space.shape\n",
    "            action_dim: env.action_space.shape[0]\n",
    "            learning_rate: learning rate for training\n",
    "            tau: update parameter for target.\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "\n",
    "    def build_network(self):\n",
    "        \"\"\"\n",
    "        build network.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"build newtork first!\")\n",
    "\n",
    "    def train(self, *args):\n",
    "        raise NotImplementedError(\"train network!\")\n",
    "\n",
    "    def predict(self, *args):\n",
    "        raise NotImplementedError(\"predict output for network!\")\n",
    "\n",
    "    def predict_target(self, *args):\n",
    "        raise NotImplementedError(\"predict output for target network!\")\n",
    "\n",
    "    def update_target_network(self):\n",
    "        raise NotImplementedError(\"update target network!\")\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        raise NotImplementedError(\"update target network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(BaseNetwork):\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau):\n",
    "        super(ActorNetwork, self).__init__(sess, state_dim, action_dim, learning_rate, tau)\n",
    "        self.action_bound = action_bound\n",
    "#         self.input_dim = input_dim\n",
    "\n",
    "        # Actor network\n",
    "        self.inputs, self.phase, self.outputs, self.scaled_outputs = self.build_network()\n",
    "        self.net_params = tf.trainable_variables()\n",
    "#         print('tf.trainable_variables()', tf.trainable_variables())\n",
    "#         print('self.inputs.shape', self.inputs.shape) # [?, 407]\n",
    "\n",
    "        # Target network\n",
    "        self.target_inputs, self.target_phase, self.target_outputs, self.target_scaled_outputs = self.build_network()\n",
    "        self.target_net_params = tf.trainable_variables()[len(self.net_params):]\n",
    "#         print('self.target_inputs.shape', self.target_inputs.shape) # [?, 407]\n",
    "\n",
    "        # Op for periodically updating target network with online network weights\n",
    "        self.update_target_net_params = \\\n",
    "            [self.target_net_params[i].assign(tf.multiply(self.net_params[i], self.tau) +\n",
    "                                              tf.multiply(self.target_net_params[i], 1. - self.tau))\n",
    "             for i in range(len(self.target_net_params))]\n",
    "\n",
    "        # Combine dnetScaledOut/dnetParams with criticToActionGradient to get actorGradient\n",
    "        # Temporary placeholder action gradient\n",
    "        self.action_gradients = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        self.actor_gradients = tf.gradients(self.outputs, self.net_params, -self.action_gradients)\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.net_params))\n",
    "\n",
    "        self.num_trainable_vars = len(self.net_params) + len(self.target_net_params)\n",
    "\n",
    "    def build_network(self):\n",
    "        inputs = tf.placeholder(tf.float32, shape=(None,) + self.state_dim)\n",
    "        phase = tf.placeholder(tf.bool)\n",
    "        net = fully_connected(inputs, 400, activation_fn=tf.nn.relu)\n",
    "        net = fully_connected(net, 300, activation_fn=tf.nn.relu)\n",
    "        # Final layer weight are initialized to Uniform[-3e-3, 3e-3]\n",
    "        outputs = fully_connected(net, 1, weights_initializer=tf.random_uniform_initializer(-3e-3, 3e-3))\n",
    "        scaled_outputs = discretize(outputs, self.action_dim)\n",
    "        print('build_network')\n",
    "\n",
    "        return inputs, phase, outputs, scaled_outputs\n",
    "\n",
    "    def train(self, *args):\n",
    "        # args [inputs, action_gradients, phase]\n",
    "        return self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: args[0],\n",
    "            self.action_gradients: args[1],\n",
    "            self.phase: True\n",
    "        })\n",
    "\n",
    "    def predict(self, *args):\n",
    "        return self.sess.run(self.scaled_outputs, feed_dict={\n",
    "            self.inputs: args[0],\n",
    "            self.phase: False\n",
    "        })\n",
    "\n",
    "    def predict_target(self, *args):\n",
    "        return self.sess.run(self.target_scaled_outputs, feed_dict={\n",
    "            self.target_inputs: args[0],\n",
    "            self.target_phase: False,\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_net_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "class CriticNetwork(BaseNetwork):\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, num_actor_vars):\n",
    "        super(CriticNetwork, self).__init__(sess, state_dim, action_dim, learning_rate, tau)\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "        # Critic network\n",
    "        self.inputs, self.phase, self.action, self.outputs = self.build_network()\n",
    "        self.net_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target network\n",
    "        self.target_inputs, self.target_phase, self.target_action, self.target_outputs = self.build_network()\n",
    "        self.target_net_params = tf.trainable_variables()[len(self.net_params) + num_actor_vars:]\n",
    "\n",
    "        # Op for periodically updating target network with online network weights\n",
    "        self.update_target_net_params = \\\n",
    "            [self.target_net_params[i].assign(tf.multiply(self.net_params[i], self.tau) +\n",
    "                                              tf.multiply(self.target_net_params[i], 1. - self.tau))\n",
    "             for i in range(len(self.target_net_params))]\n",
    "\n",
    "        self.update_target_bn_params = \\\n",
    "            [self.target_net_params[i].assign(self.net_params[i]) for i in range(len(self.target_net_params)) if self.target_net_params[i].name.startswith('BatchNorm')]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        # Obtained from the target networks\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.predicted_q_value, self.outputs))\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the critic w.r.t. the action\n",
    "        self.action_grads = tf.gradients(self.outputs, self.action)\n",
    "\n",
    "    def build_network(self):\n",
    "        inputs = tf.placeholder(tf.float32, shape=(None,) + self.state_dim)\n",
    "        phase = tf.placeholder(tf.bool)\n",
    "        action = tf.placeholder(tf.float32, [None, 1])\n",
    "        net = fully_connected(inputs, 400, activation_fn=tf.nn.relu)\n",
    "        net = fully_connected(tf.concat([net, action], 1), 300, activation_fn=tf.nn.relu)\n",
    "        outputs = fully_connected(net, 1, weights_initializer=tf.random_uniform_initializer(-3e-3, 3e-3))\n",
    "\n",
    "        return inputs, phase, action, outputs\n",
    "\n",
    "    def train(self, *args):\n",
    "        # args (inputs, action, predicted_q_value, phase)\n",
    "        return self.sess.run([self.outputs, self.optimize], feed_dict={\n",
    "            self.inputs: args[0],\n",
    "            self.action: args[1],\n",
    "            self.predicted_q_value: args[2],\n",
    "            self.phase: True\n",
    "        })\n",
    "\n",
    "    def predict(self, *args):\n",
    "        # args  (inputs, action, phase)\n",
    "        return self.sess.run(self.outputs, feed_dict={\n",
    "            self.inputs: args[0],\n",
    "            self.action: args[1],\n",
    "            self.phase: False\n",
    "        })\n",
    "\n",
    "    def predict_target(self, *args):\n",
    "        # args  (inputs, action, phase)\n",
    "        return self.sess.run(self.target_outputs, feed_dict={\n",
    "            self.target_inputs: args[0],\n",
    "            self.target_action: args[1],\n",
    "            self.target_phase: False\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, action):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.phase: False\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_net_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent(object):\n",
    "    def __init__(self, sess, env, replay_buffer, noise=None, exploration_episodes=10000, max_episodes=10000, max_steps_episode=10000, warmup_steps=5000,\\\n",
    "            mini_batch=32, eval_episodes=10, eval_periods=100, env_render=False, summary_dir=None):\n",
    "        \"\"\"\n",
    "        Base agent, provide basic functions. \n",
    "        Args:\n",
    "            sess: tf.Session(). \n",
    "            env: openai gym environment. could be a wrapper.\n",
    "            replay_buffer: replay_buffer for sampling. \n",
    "            noise: noise added to action for exploration. \n",
    "            exploration_episodes: maximum episodes for training with noise.\n",
    "            max_episodes: maximum episodes for training.\n",
    "            max_steps_episode: maximum steps per episode.\n",
    "            mini_batch: mini batch size in the training.\n",
    "            eval_episodes: number of episodes to evaluate current model.\n",
    "            eval_periods: periods to evaluate model.\n",
    "            env_render: whether display observation.\n",
    "            summary_dir: folder to store summaries of algorithm.\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.noise = noise\n",
    "        self.exploration_episodes = exploration_episodes\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_steps_episode = max_steps_episode\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.mini_batch = mini_batch\n",
    "        self.eval_episodes = eval_episodes\n",
    "        self.eval_periods = eval_periods\n",
    "        self.env_render = env_render\n",
    "        self.summary_dir = summary_dir\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.writer = tf.summary.FileWriter(self.summary_dir, sess.graph)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"train() method should be implemented\")\n",
    "\n",
    "\n",
    "    def evaluate(self, cur_episode):\n",
    "        \"\"\"\n",
    "        evaluate the model.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"evaluate() method should be implemented\")\n",
    "        \n",
    "class DDPGAgent(BaseAgent):\n",
    "    def __init__(self, sess, actor, critic, gamma, env, replay_buffer, noise=None, exploration_episodes=10000, max_episodes=10000, max_steps_episode=10000,\\\n",
    "            warmup_steps=5000, mini_batch=32, eval_episodes=10, eval_periods=100, env_render=False, summary_dir=None):\n",
    "        \"\"\"\n",
    "        Deep Deterministic Policy Gradient Agent.\n",
    "        Args:\n",
    "            actor: actor network.\n",
    "            critic: critic network.\n",
    "            gamma: discount factor.\n",
    "        \"\"\"\n",
    "        super(DDPGAgent, self).__init__(sess, env, replay_buffer, noise=noise, exploration_episodes=exploration_episodes, max_episodes=max_episodes, max_steps_episode=max_steps_episode,\\\n",
    "                warmup_steps=warmup_steps, mini_batch=mini_batch, eval_episodes=eval_episodes, eval_periods=eval_periods, env_render=env_render, summary_dir=summary_dir)\n",
    "\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize target network weights\n",
    "        self.actor.update_target_network()\n",
    "        self.critic.update_target_network()\n",
    "\n",
    "        for cur_episode in tqdm(xrange(self.max_episodes)):\n",
    "\n",
    "            # evaluate here. \n",
    "            if cur_episode % self.eval_periods == 0:\n",
    "                self.evaluate(cur_episode)\n",
    "\n",
    "            state = self.env.reset()\n",
    "#             state = state_to_matrix(state[0])\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_ave_max_q = 0\n",
    "\n",
    "            for cur_step in xrange(self.max_steps_episode):\n",
    "\n",
    "                if self.env_render:\n",
    "                    self.env.render()\n",
    "\n",
    "                # Add exploratory noise according to Ornstein-Uhlenbeck process to action\n",
    "                if self.replay_buffer.size() < self.warmup_steps:\n",
    "                    action= self.env.action_space.sample()\n",
    "#                     action = action[0]\n",
    "                else: \n",
    "                    action = self.noise.generate(self.actor.predict(np.expand_dims(state, 0))[0,0], cur_episode)\n",
    "#                     actoin = action[0]\n",
    "\n",
    "                next_state, reward, terminal, info = self.env.step(action)\n",
    "#                 next_state = state_to_matrix(next_state[0])\n",
    "\n",
    "                self.replay_buffer.add(state, action, reward, terminal, next_state)\n",
    "\n",
    "                # Keep adding experience to the memory until there are at least minibatch size samples\n",
    "                if self.replay_buffer.size() > self.warmup_steps:\n",
    "                    state_batch, action_batch, reward_batch, terminal_batch, next_state_batch = \\\n",
    "                        self.replay_buffer.sample_batch(self.mini_batch)\n",
    "\n",
    "                    # Calculate targets\n",
    "                    target_q = self.critic.predict_target(next_state_batch, self.actor.predict_target(next_state_batch))\n",
    "\n",
    "                    y_i = np.reshape(reward_batch, (self.mini_batch, 1)) + (1 \\\n",
    "                            - np.reshape(terminal_batch, (self.mini_batch, 1)).astype(float))\\\n",
    "                            * self.gamma * np.reshape(target_q, (self.mini_batch, 1))\n",
    "\n",
    "                    # Update the critic given the targets\n",
    "                    action_batch = np.reshape(action_batch, [self.mini_batch, 1])\n",
    "\n",
    "                    episode_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                    # Update the actor policy using the sampled gradient\n",
    "                    a_outs = self.actor.predict(state_batch)\n",
    "                    a_grads = self.critic.action_gradients(state_batch, a_outs)\n",
    "                    self.actor.train(state_batch, a_grads[0])\n",
    "\n",
    "\n",
    "                    # Update target networks\n",
    "                    self.actor.update_target_network()\n",
    "                    self.critic.update_target_network()\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if terminal or cur_step == self.max_steps_episode-1:\n",
    "                    train_episode_summary = tf.Summary() \n",
    "                    train_episode_summary.value.add(simple_value=episode_reward, tag=\"train/episode_reward\")\n",
    "                    train_episode_summary.value.add(simple_value=episode_ave_max_q/float(cur_step), tag=\"train/episode_ave_max_q\")\n",
    "                    self.writer.add_summary(train_episode_summary, cur_episode)\n",
    "                    self.writer.flush()\n",
    "\n",
    "                    print('Reward: %.2i' % int(episode_reward), ' | Episode', cur_episode, \\\n",
    "                          '| Qmax: %.4f' % (episode_ave_max_q / float(cur_step)))\n",
    "\n",
    "                    break\n",
    "\n",
    "\n",
    "    def evaluate(self, cur_episode):\n",
    "        # evaluate here. \n",
    "        total_episode_reward = 0 \n",
    "        for eval_i in xrange(self.eval_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = state.flatten()\n",
    "#             state = state_to_matrix(state[0])\n",
    "            \n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                action = self.actor.predict(np.expand_dims(state, 0))[0,0]\n",
    "#                 print('action', action)\n",
    "                \n",
    "                state, reward, terminal, info = self.env.step(action)\n",
    "#                 print('state', state)\n",
    "                total_episode_reward += reward\n",
    "        ave_episode_reward = total_episode_reward / float(self.eval_episodes)\n",
    "        print(\"\\nAverage reward {}\\n\".format(ave_episode_reward))\n",
    "        # Add ave reward to Tensorboard\n",
    "        eval_episode_summary = tf.Summary()\n",
    "        eval_episode_summary.value.add(simple_value=ave_episode_reward, tag=\"eval/reward\")\n",
    "        self.writer.add_summary(eval_episode_summary, cur_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_matrix_2(obs):\n",
    "    #In this implementation I just concatenate everything in one big matrix\n",
    "\n",
    "    #for e in obs['enemies']:\n",
    "    #    print(e)\n",
    "    #    print(Item(e))\n",
    "    #TODO enemies\n",
    "    my_position = np.asmatrix(obs['position'])\n",
    "    bomb_life = np.asmatrix(obs['bomb_life'])\n",
    "    board = np.asmatrix(obs['board'])\n",
    "    bombs = np.asmatrix(obs['bomb_blast_strength'])\n",
    "    #enemies = np.asmatrix([Item_en(e) for e in obs['enemies']])\n",
    "    can_kick = np.asmatrix(int(1 if obs['can_kick'] else 0))\n",
    "    ammo = np.asmatrix(int(obs['ammo']))\n",
    "    blast_strength = np.asmatrix(int(obs['blast_strength']))\n",
    "\n",
    "    m = np.max([my_position.shape[1], bomb_life.shape[1], board.shape[1], bombs.shape[1],  can_kick.shape[1], ammo.shape[1], blast_strength.shape[1]])\n",
    "\n",
    "    my_position1 = np.concatenate((my_position, np.zeros(( my_position.shape[0], m - my_position.shape[1]))), axis=1)\n",
    "    bomb_life1 = np.concatenate((bomb_life, np.zeros((bomb_life.shape[0], m - bomb_life.shape[1]))), axis=1)\n",
    "    board1 = np.concatenate((board, np.zeros((board.shape[0], m - board.shape[1]))), axis=1)\n",
    "    bombs1 = np.concatenate((bombs, np.zeros((bombs.shape[0], m - bombs.shape[1]))), axis=1)\n",
    "    #enemies1 = np.concatenate((enemies, np.zeros((enemies.shape[0], m - enemies.shape[1]))), axis=1)\n",
    "    can_kick1 = np.concatenate((can_kick, np.zeros((can_kick.shape[0], m - can_kick.shape[1]))), axis=1)\n",
    "    ammo1 = np.concatenate((ammo, np.zeros((ammo.shape[0], m - ammo.shape[1]))), axis=1)\n",
    "    blast_strength1 = np.concatenate((blast_strength, np.zeros((blast_strength.shape[0], m - blast_strength.shape[1]))), axis=1)\n",
    "\n",
    "    result = np.concatenate((my_position1, bomb_life1, board1, bombs1, can_kick1, ammo1, blast_strength1), axis=0)\n",
    "    return np.asmatrix(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## Trying to concatenate without extra padding (but has to be same dimension -- failed)\n",
    "\n",
    "# state = env.reset()\n",
    "# print(state_to_matrix(state[0]).shape)\n",
    "# my_position1 = np.asmatrix(state[0]['position'])\n",
    "# bomb_life1=np.asmatrix(state[0]['bomb_life'])\n",
    "# board1=np.asmatrix(state[0]['board'])\n",
    "# bombs1=np.asmatrix(state[0]['bomb_blast_strength'])\n",
    "# can_kick1=np.asmatrix(int(1 if state[0]['can_kick'] else 0))\n",
    "# ammo1=np.asmatrix(state[0]['ammo'])\n",
    "# blast_strength1=np.asmatrix(state[0]['blast_strength'])\n",
    "# result = np.concatenate((my_position1, bomb_life1, board1, bombs1, can_kick1, ammo1, blast_strength1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.n\n",
    "action_bound = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = env_wrapper\n",
    "\n",
    "state = env.reset()\n",
    "# state = state_to_matrix(state[0])\n",
    "# input_dim = state.flatten().shape\n",
    "# input_dim = (input_dim[-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_network\n",
      "build_network\n"
     ]
    }
   ],
   "source": [
    "actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                     ACTOR_LEARNING_RATE, TAU)\n",
    "critic = CriticNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                       CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())\n",
    "\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "noise = GreedyPolicy(action_dim, EXPLORATION_EPISODES, MIN_EPSILON, MAX_EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(sess, actor, critic, GAMMA, env, replay_buffer, noise=noise, \\\n",
    "                  exploration_episodes=EXPLORATION_EPISODES,max_episodes=MAX_EPISODES, \\\n",
    "                  max_steps_episode=MAX_STEPS_EPISODE, warmup_steps=WARMUP_STEPS,mini_batch=MINI_BATCH,\\\n",
    "                  eval_episodes=EVAL_EPISODES, eval_periods=EVAL_PERIODS, env_render=False, summary_dir=SUMMARY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 2057) for Tensor 'Placeholder_115:0', which has shape '(?, 372)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-3d92c9b1ff97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-233-6752a3308638>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# evaluate here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcur_episode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_periods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-233-6752a3308638>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, cur_episode)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;31m#                 print('action', action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-1bf6db011881>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     55\u001b[0m         return self.sess.run(self.scaled_outputs, feed_dict={\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         })\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1111\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1112\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 2057) for Tensor 'Placeholder_115:0', which has shape '(?, 372)'"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * I think the dimension of input type should be (?, 372).\n",
    "### * In the networks, the required parameters are (?, 372); the input parameters are (1, 37, 11) -- state_to_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY TO RUN LINE BY LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTOR INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(state_dim):\n",
    "    inputs = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
    "    phase = tf.placeholder(tf.bool)\n",
    "    net = fully_connected(inputs, 400, activation_fn=tf.nn.relu)\n",
    "    net = fully_connected(net, 300, activation_fn=tf.nn.relu)\n",
    "    # Final layer weight are initialized to Uniform[-3e-3, 3e-3]\n",
    "    outputs = fully_connected(net, 1, weights_initializer=tf.random_uniform_initializer(-3e-3, 3e-3))\n",
    "    scaled_outputs = discretize(outputs, action_dim)\n",
    "    \n",
    "    return inputs, phase, outputs, scaled_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainputs, aphase, aoutputs, ascaled_outputs = build_network(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "anet_params = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "atarget_inputs, atarget_phase, atarget_outputs, atarget_scaled_outputs = build_network(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "atarget_net_params = tf.trainable_variables()[len(anet_params):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "aupdate_target_net_params = [atarget_net_params[i].assign(tf.multiply(anet_params[i], TAU) + \n",
    "                                                        tf.multiply(atarget_net_params[i], 1. - TAU))\n",
    "                            for i in range(len(atarget_net_params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_gradients = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_gradients = tf.gradients(outputs, anet_params, -action_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoptimize = tf.train.AdamOptimizer(ACTOR_LEARNING_RATE).\\\n",
    "            apply_gradients(zip(actor_gradients, anet_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_vars = len(anet_params) + len(atarget_net_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRITIC INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinputs, aphase, caction, cscaled_outputs = build_network(state_dim)\n",
    "ctarget_inputs, ctarget_phase, ctarget_action, ctarget_scaled_outputs = build_network(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnet_params = tf.trainable_variables()[num_trainable_vars:]\n",
    "ctarget_net_params = tf.trainable_variables()[len(cnet_params) + num_trainable_vars:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cupdate_target_net_params = [ctarget_net_params[i].assign(tf.multiply(cnet_params[i], TAU) +\n",
    "                                              tf.multiply(ctarget_net_params[i], 1. - TAU))\n",
    "             for i in range(len(ctarget_net_params))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_q_value = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.squared_difference(predicted_q_value, outputs))\n",
    "coptimize = tf.train.AdamOptimizer(CRITIC_LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_grads = tf.gradients(coutputs, caction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "noise = GreedyPolicy(action_dim, EXPLORATION_EPISODES, MIN_EPSILON, MAX_EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENT TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value fully_connected_1/weights\n\t [[Node: fully_connected_1/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@fully_connected_1/weights\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fully_connected_1/weights)]]\n\nCaused by op 'fully_connected_1/weights/read', defined at:\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-b85f0a18e16c>\", line 4, in <module>\n    net = fully_connected(net, 300, activation_fn=tf.nn.relu)\n  File \"<ipython-input-5-84b2a229ab53>\", line 32, in fully_connected\n    return tf.contrib.layers.fully_connected(inputs, output_size, activation_fn=activation_fn,             weights_initializer=weights_initializer, weights_regularizer=weights_regularizer, biases_initializer=biases_initializer)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1716, in fully_connected\n    outputs = layer.apply(inputs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 828, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 699, in __call__\n    self.build(input_shapes)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 138, in build\n    trainable=True)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 546, in add_variable\n    partitioner=partitioner)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\", line 436, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\n    constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\n    constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 417, in get_variable\n    return custom_getter(**custom_getter_kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1611, in layer_variable_getter\n    return _model_variable_getter(getter, *args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1602, in _model_variable_getter\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 291, in model_variable\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 246, in variable\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 786, in _get_single_variable\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2220, in variable\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2210, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2193, in default_variable_creator\n    constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 397, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 142, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3187, in identity\n    \"Identity\", input=input, name=name)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value fully_connected_1/weights\n\t [[Node: fully_connected_1/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@fully_connected_1/weights\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fully_connected_1/weights)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value fully_connected_1/weights\n\t [[Node: fully_connected_1/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@fully_connected_1/weights\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fully_connected_1/weights)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-8496722318a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maupdate_target_net_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value fully_connected_1/weights\n\t [[Node: fully_connected_1/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@fully_connected_1/weights\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fully_connected_1/weights)]]\n\nCaused by op 'fully_connected_1/weights/read', defined at:\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-b85f0a18e16c>\", line 4, in <module>\n    net = fully_connected(net, 300, activation_fn=tf.nn.relu)\n  File \"<ipython-input-5-84b2a229ab53>\", line 32, in fully_connected\n    return tf.contrib.layers.fully_connected(inputs, output_size, activation_fn=activation_fn,             weights_initializer=weights_initializer, weights_regularizer=weights_regularizer, biases_initializer=biases_initializer)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1716, in fully_connected\n    outputs = layer.apply(inputs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 828, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 699, in __call__\n    self.build(input_shapes)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 138, in build\n    trainable=True)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 546, in add_variable\n    partitioner=partitioner)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\", line 436, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\n    constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\n    constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 417, in get_variable\n    return custom_getter(**custom_getter_kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1611, in layer_variable_getter\n    return _model_variable_getter(getter, *args, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1602, in _model_variable_getter\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 291, in model_variable\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 246, in variable\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 786, in _get_single_variable\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2220, in variable\n    use_resource=use_resource)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2210, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2193, in default_variable_creator\n    constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 397, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 142, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3187, in identity\n    \"Identity\", input=input, name=name)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/Users/teggsung/anaconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value fully_connected_1/weights\n\t [[Node: fully_connected_1/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@fully_connected_1/weights\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fully_connected_1/weights)]]\n"
     ]
    }
   ],
   "source": [
    "sess.run(aupdate_target_net_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf36]",
   "language": "python",
   "name": "conda-env-tf36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
